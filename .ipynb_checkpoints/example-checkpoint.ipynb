{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGEu0sSVtn1q"
   },
   "source": [
    "# XGBoost Regression with TensorFlow Pooling and Loss\n",
    "## Intro\n",
    "Consider features are available on Individual level, predictions are required also on the Individual level but target is available for Groups of Individuals only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kplQA4_N0dKt"
   },
   "source": [
    "![picture](img/arch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHStvvROtn1t"
   },
   "source": [
    "Predictions of XGBoost on the Individual level will be pooled to the Group level using a custom TensorFlow function. The same function uses one of TensorFlow losses to calculate the final scalar loss by comparing the target  on Group level with the pooled predictions to the Group level.\n",
    "\n",
    "The goal is to provide a decorator, which turns the mentioned TensorFlow pooling and loss function to the XGBoost custom objective function, such that the whole aggregation and calculation of the 1st and 2nd order derivatives is done seamlessly during XGBoost training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NAD9Wlibtn11"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1604e2afefe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf2xgb import get_ragged_nested_index_lists, gen_random_dataset, xgb_tf_loss\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxOn24zltn10"
   },
   "source": [
    "## Dummy Input Dataset\n",
    "Let's generate random \"observed\" data incl. targets on Individual level. Then, add aggregated targets on Subgroup and Group levels. In the end, we will be able to compare estimates on the Individual-level targets, which is not available in practice in the example above, with the estimates on Subgroup- and Group-level targets.\n",
    "\n",
    "Note that the aggregation from Individual to Subgroup level is MAX, and the aggregation from the Subgroup to Group level is SUM in this Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAb934zWtn16"
   },
   "outputs": [],
   "source": [
    "N = 100000\n",
    "N_TEST = 10000\n",
    "N_SUBGRP = N//2\n",
    "N_GRP = N_SUBGRP//2\n",
    "BETA_TRUE = [2,1,0,0,0]\n",
    "SIGMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwZVbGWJITmd"
   },
   "outputs": [],
   "source": [
    "# main data frame with features X, subgroup IDs subgrp_id and group ID grp_id;\n",
    "# target y is NOT observable on the individual level in real data,\n",
    "# we have it here to be able to simulate target on group level\n",
    "# and to be able to compared result of the estimate on the group-level\n",
    "# target with the estimate on the individual level.\n",
    "df_train = gen_random_dataset(N, N_SUBGRP, N_GRP, BETA_TRUE, SIGMA)\n",
    "df_test = gen_random_dataset(N_TEST, 0, 0, BETA_TRUE, SIGMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "PbKGNt3hDHpJ",
    "outputId": "473d7ac3-5572-4376-e53c-dca4473c312a"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ow_-iiiBavN"
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(df_train['X'].to_list())\n",
    "y_train = np.asarray(df_train['y'].to_list())\n",
    "X_test = np.asarray(df_test['X'].to_list())\n",
    "y_test = np.asarray(df_test['y'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdnhJeiiYQch"
   },
   "source": [
    "Calculate simulated target `y` on the level of `subgrp_id` (by max pooling of individual-level `y`'s) and `grp_id` (by sum of `subgrp_id`-level `y`'s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZskJ767tQxY-"
   },
   "outputs": [],
   "source": [
    "df_train_subgrp_y = (df_train\n",
    "    .groupby('subgrp_id')\n",
    "    .agg({'y':np.max, 'grp_id':max})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V81rtksiRWbj"
   },
   "outputs": [],
   "source": [
    "df_train_grp_y = (df_train_subgrp_y\n",
    "    .groupby('grp_id')\n",
    "    .agg({'y':np.sum})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSDiRlJYMDNh"
   },
   "outputs": [],
   "source": [
    "df_train_subgrp_inds = get_ragged_nested_index_lists(df_train, ['subgrp_id'])\n",
    "df_train_grp_inds = get_ragged_nested_index_lists(df_train, ['grp_id', 'subgrp_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG4ElCD-mc4u"
   },
   "source": [
    "## Custom TF Pooling and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV0iQrHC9de-"
   },
   "outputs": [],
   "source": [
    "@xgb_tf_loss(df_train_subgrp_inds.sort_values(by=['subgrp_id'])['_row_'].to_list(), \n",
    "             df_train_subgrp_y.sort_values(by=['subgrp_id'])['y'].to_numpy())\n",
    "def xgb_subgrp_obj_fn_from_tf(target, preds_cube):\n",
    "    \"\"\"Custom TF Pooling and Loss function.\n",
    "\n",
    "    Inputs:\n",
    "    = target: 1D tensor with target on the level of groups\n",
    "    = preds_cube: ND tensor with predictions on the individual level;\n",
    "    the first dimension is that of groups, the other dimensions reflect\n",
    "    sub-groups on different levels and individual observations\n",
    "    (target.shape[0] == preds_cube.shape[0]; \n",
    "    preds_cube.shape[-1] == max # indiv observations per the most detailed \n",
    "    sub-group).\n",
    "    Missing values are denoted by np.nan and have to be taken care of in \n",
    "    this function body. They occur simply because preds_cube\n",
    "    has typically much more elements that the original flat predictions\n",
    "    vector from XGBoost.\n",
    "\n",
    "    Output: scalar tensor reflecting MEAN of losses over all dimensions.\n",
    "    This is the output of e.g. tf.keras.losses.mean_squared_error().\n",
    "    The mean is translated to SUM later in tf_d_loss() because of the \n",
    "    compatibility with XGB custom objective function.\n",
    "    \"\"\"\n",
    "    x = preds_cube\n",
    "    # replace NaNs with -Inf: neutral value for reduce_max()\n",
    "    x = tf.where(tf.math.is_nan(x), tf.constant(-np.inf, dtype=x.dtype), x)\n",
    "    x = tf.math.reduce_max(x, axis=-1)\n",
    "    l = tf.keras.losses.mean_squared_error(target, x)\n",
    "    return l\n",
    "\n",
    "\n",
    "@xgb_tf_loss(df_train_grp_inds.sort_values(by=['grp_id'])['_row_'].to_list(), \n",
    "             df_train_grp_y.sort_values(by=['grp_id'])['y'].to_numpy())\n",
    "def xgb_grp_obj_fn_from_tf(target, preds_cube):\n",
    "    \"\"\"Custom TF Pooling and Loss function.\n",
    "\n",
    "    Inputs:\n",
    "    = target: 1D tensor with target on the level of groups\n",
    "    = preds_cube: ND tensor with predictions on the individual level;\n",
    "    the first dimension is that of groups, the other dimensions reflect\n",
    "    sub-groups on different levels and individual observations\n",
    "    (target.shape[0] == preds_cube.shape[0]; \n",
    "    preds_cube.shape[-1] == max # indiv observations per the most detailed \n",
    "    sub-group)\n",
    "    Missing values are denoted by np.nan and have to be taken care of in \n",
    "    this function body. They occur simply because preds_cube\n",
    "    has typically much more elements that the original flat predictions\n",
    "    vector from XGBoost.\n",
    "\n",
    "    Output: scalar tensor reflecting MEAN of losses over all dimensions.\n",
    "    This is the output of e.g. tf.keras.losses.mean_squared_error().\n",
    "    The mean is translated to SUM later in tf_d_loss() because of the \n",
    "    compatibility with XGB custom objective function.\n",
    "    \"\"\"\n",
    "    x = preds_cube\n",
    "    # replace NaNs with -Inf: neutral value for reduce_max()\n",
    "    x = tf.where(tf.math.is_nan(x), tf.constant(-np.inf, dtype=x.dtype), x)\n",
    "    x = tf.math.reduce_max(x, axis=-1)\n",
    "    # replace (-)Inf's (=missing values from reduce_max()) with 0's: \n",
    "    # neutral value for reduce_sum()\n",
    "    x = tf.where(tf.math.is_inf(x), tf.constant(0, dtype=x.dtype), x)\n",
    "    x = tf.math.reduce_sum(x, axis=-1)\n",
    "    l = tf.keras.losses.mean_squared_error(target, x)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OTMkWi_tn2M"
   },
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_P8hCDrtn2V"
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL3pwaKtxQVe",
    "outputId": "d3b96227-c2d2-48cb-f621-0a81082432be"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# labels on Group level are inputs of grouped_objective(), \n",
    "# they are not part of dtrain DMatrix\n",
    "dtrain_subgrp = xgb.DMatrix(X_train)    \n",
    "regr_subgrp = xgb.train({'tree_method': 'hist', 'seed': 1994},  # any other tree method is fine.\n",
    "           dtrain=dtrain_subgrp,\n",
    "           num_boost_round=10,\n",
    "           obj=xgb_subgrp_obj_fn_from_tf)\n",
    "# predictions are on Individual level despite the target on Group level\n",
    "y_subgrp = regr_subgrp.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyKF8hSTH303",
    "outputId": "d30640cc-d740-41d6-d5fc-9b692b5617ca"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# labels on Group level are inputs of grouped_objective(), \n",
    "# they are not part of dtrain DMatrix\n",
    "dtrain_grp = xgb.DMatrix(X_train)    \n",
    "regr_grp = xgb.train({'tree_method': 'hist', 'seed': 1994},  # any other tree method is fine.\n",
    "           dtrain=dtrain_grp,\n",
    "           num_boost_round=10,\n",
    "           obj=xgb_grp_obj_fn_from_tf)\n",
    "# predictions are on Individual level despite the target on Group level\n",
    "y_grp = regr_grp.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyv956_rtn2a"
   },
   "outputs": [],
   "source": [
    "dtrain_indiv = xgb.DMatrix(X_train, label=y_train)\n",
    "regr_indiv = xgb.train({'tree_method': 'hist', 'seed': 1994},  # any other tree method is fine.\n",
    "           dtrain=dtrain_indiv,\n",
    "           num_boost_round=10\n",
    "           )\n",
    "y_indiv = regr_indiv.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTudG3m7tn2d"
   },
   "source": [
    "## Results\n",
    "First, plot the true values vs predictions of both models on the Individual level to see the prediction accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytBmf139tn2d",
    "outputId": "7831ae01-53d2-49d4-8c4a-b16e61163396"
   },
   "outputs": [],
   "source": [
    "print(f\"MSE of individual predictions based on grp_id-pooled targets   : \"\n",
    "      f\"{mean_squared_error(y_test, y_grp)}\")\n",
    "print(f\"MSE of individual predictions based on subgrp_id-pooled targets: \"\n",
    "      f\"{mean_squared_error(y_test, y_subgrp)}\")\n",
    "print(f\"MSE of individual predictions based on individual targets      : \"\n",
    "      f\"{mean_squared_error(y_test, y_indiv)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_grp, color=\"red\", label=\"grp_id pooled targets\", linewidth=2)\n",
    "plt.scatter(y_test, y_subgrp, color=\"blue\", label=\"subgrp_id pooled targets\", linewidth=2)\n",
    "plt.scatter(y_test, y_indiv, color=\"green\", label=\"individual targets\", linewidth=2)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"pred\")\n",
    "plt.title(\"XGBoost Regression: true vs predicted values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VtAnL7Ftn2g"
   },
   "source": [
    "The predictions on targets on different levels (individual, subgroup, group) are similarly precise compared to true individual-level target values.\n",
    "\n",
    "In ideal case, predictions on the Subgroup- and Group-level targets would be equal to the predictions on Individual-level target. Let's check similarity of both predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51U-K4Iqtn2g",
    "outputId": "95740bf0-c2ca-45a4-cc12-4c384633aba9"
   },
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_indiv, y_subgrp))\n",
    "print(mean_squared_error(y_indiv, y_grp))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_indiv, y_grp, color=\"red\", label=\"grp_id pooled targets\", linewidth=2)\n",
    "plt.scatter(y_indiv, y_subgrp, color=\"green\", label=\"subgrp_id pooled targets\", linewidth=2)\n",
    "plt.xlabel(\"y_indiv\")\n",
    "plt.ylabel(\"y_subgrp, y_grp\")\n",
    "plt.title(\"XGBoost Regression: predictions on individual vs gruped targets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKapazZ72Fu3"
   },
   "source": [
    "We can see that individual-level predictions using (sub)group-level targets are close to those using the individual-level target, which is in our real setup not available."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XGBoost_with_TensorFlow_pooling_and_loss.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
